{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import string\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import nltk\r\n",
    "nltk.download('stopwords')\r\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\r\n",
    "import re"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\renat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Base de Dados de Treino"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "df = pd.read_csv('../data/train/Tweets_Mg.csv')\r\n",
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8199, 26)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# remoção de features inrelevantes\r\n",
    "df = df.drop(columns=['Unnamed: 0', 'Created At', 'Geo Coordinates.latitude',\r\n",
    "       'Geo Coordinates.longitude', 'User Location', 'Username',\r\n",
    "       'User Screen Name', 'Retweet Count', 'Observação',\r\n",
    "       'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\r\n",
    "       'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17',\r\n",
    "       'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21',\r\n",
    "       'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "df.Classificacao.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Positivo    3300\n",
       "Neutro      2453\n",
       "Negativo    2446\n",
       "Name: Classificacao, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df.Text"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       ���⛪ @ Catedral de Santo Antônio - Governador ...\n",
       "1       � @ Governador Valadares, Minas Gerais https:/...\n",
       "2       �� @ Governador Valadares, Minas Gerais https:...\n",
       "3                             ��� https://t.co/BnDsO34qK0\n",
       "4       ��� PSOL vai questionar aumento de vereadores ...\n",
       "                              ...                        \n",
       "8194    Trio é preso suspeito de roubo, tráfico e abus...\n",
       "8195    Trio é preso suspeito de roubo, tráfico e abus...\n",
       "8196    Trio é preso suspeito de roubo, tráfico e abus...\n",
       "8197    Trio é preso suspeito de roubo, tráfico e abus...\n",
       "8198    Trio suspeito de roubo de cargas é preso em Sa...\n",
       "Name: Text, Length: 8199, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\r\n",
    "\r\n",
    "### Features que serão adicionadas\r\n",
    "    1 - Número de palavras no texto\r\n",
    "    2 - Número de palavras únicas no texto\r\n",
    "    3 - Número de caracteres no texto\r\n",
    "    4 - Número de palavras irrelevantes (stopwords)\r\n",
    "    5 - Número de pontuações\r\n",
    "    6 - Número palavras maiúsculas\r\n",
    "    7 - Número de palavras de caixa de título\r\n",
    "    8 - Comprimento médio das palavras\r\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Numero de palavras no texto\r\n",
    "df[\"num_words\"] = df['Text'].apply(lambda x: len(str(x).split()))\r\n",
    "\r\n",
    "# Numero de palavras unicas no texto\r\n",
    "df['num_unique_words'] = df['Text'].apply(lambda x: len(set(str(x).split())))\r\n",
    "\r\n",
    "# Número de caracteres no texto\r\n",
    "df['num_chars'] = df['Text'].apply(lambda x: len(str(x)))\r\n",
    "\r\n",
    "# Número de palavras irrelevantes (stopwords)\r\n",
    "df['num_stopwords'] = df['Text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\r\n",
    "\r\n",
    "# Número de pontuações\r\n",
    "df['num_punctuations'] = df['Text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\r\n",
    "\r\n",
    "# Número palavras maiúsculas\r\n",
    "df['num_words_upper'] = df['Text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\r\n",
    "\r\n",
    "# Número de palavras de caixa de título\r\n",
    "df['num_words_title'] = df['Text'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\r\n",
    "\r\n",
    "# Comprimento médio das palavras\r\n",
    "df['mean_word_len'] = df['Text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Classificacao</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>��� https://t.co/BnDsO34qK0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>126</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Classificacao  num_words  \\\n",
       "0  ���⛪ @ Catedral de Santo Antônio - Governador ...        Neutro         10   \n",
       "1  � @ Governador Valadares, Minas Gerais https:/...        Neutro          7   \n",
       "2  �� @ Governador Valadares, Minas Gerais https:...        Neutro          7   \n",
       "3                        ��� https://t.co/BnDsO34qK0        Neutro          2   \n",
       "4  ��� PSOL vai questionar aumento de vereadores ...      Negativo         20   \n",
       "\n",
       "   num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0                10         82              1                 8   \n",
       "1                 7         62              0                 7   \n",
       "2                 7         63              0                 7   \n",
       "3                 2         27              0                 5   \n",
       "4                17        126              5                 7   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                0                4       7.300000  \n",
       "1                0                4       8.000000  \n",
       "2                0                4       8.142857  \n",
       "3                0                0      13.000000  \n",
       "4                2                4       5.350000  "
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8199 entries, 0 to 8198\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Text              8199 non-null   object \n",
      " 1   Classificacao     8199 non-null   object \n",
      " 2   num_words         8199 non-null   int64  \n",
      " 3   num_unique_words  8199 non-null   int64  \n",
      " 4   num_chars         8199 non-null   int64  \n",
      " 5   num_stopwords     8199 non-null   int64  \n",
      " 6   num_punctuations  8199 non-null   int64  \n",
      " 7   num_words_upper   8199 non-null   int64  \n",
      " 8   num_words_title   8199 non-null   int64  \n",
      " 9   mean_word_len     8199 non-null   float64\n",
      "dtypes: float64(1), int64(7), object(2)\n",
      "memory usage: 640.7+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\r\n",
    "- Converter texto para minúsculo\r\n",
    "- Remover pontuações\r\n",
    "- Limpeza de Número \r\n",
    "- remoção de stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remoção de pontuação"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Removing punctuations\r\n",
    "punctuation_list =[',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \r\n",
    "        '•', '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', \r\n",
    "        '█', '…', '“', '★', '”', '–', '●', '►', '−', '¢', '¬', '░', '¡', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', \r\n",
    "        '—', '‹', '─', '▒', '：', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '¸', '⋅', '‘', '∞', \r\n",
    "        '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '・', '╦', '╣', '╔', '╗', '▬', '❤', '≤', '‡', '√', '◄', '━', \r\n",
    "        '⇒', '▶', '≥', '╝', '♡', '◊', '。', '✈', '≡', '☺', '✔', '↵', '≈', '✓', '♣', '☎', '℃', '◦', '└', '‟', '～', '！', '○', \r\n",
    "        '◆', '№', '♠', '▌', '✿', '▸', '⁄', '□', '❖', '✦', '．', '÷', '｜', '┃', '／', '￥', '╠', '↩', '✭', '▐', '☼', '☻', '┐', \r\n",
    "        '├', '«', '∼', '┌', '℉', '☮', '฿', '≦', '♬', '✧', '〉', '－', '⌂', '✖', '･', '◕', '※', '‖', '◀', '‰', '\\x97', '↺', \r\n",
    "        '∆', '┘', '┬', '╬', '،', '⌘', '⊂', '＞', '〈', '⎙', '？', '☠', '⇐', '▫', '∗', '∈', '≠', '♀', '♔', '˚', '℗', '┗', '＊', \r\n",
    "        '┼', '❀', '＆', '∩', '♂', '‿', '∑', '‣', '➜', '┛', '⇓', '☯', '⊖', '☀', '┳', '；', '∇', '⇑', '✰', '◇', '♯', '☞', '´', \r\n",
    "        '↔', '┏', '｡', '◘', '∂', '✌', '♭', '┣', '┴', '┓', '✨', '\\xa0', '˜', '❥', '┫', '℠', '✒', '［', '∫', '\\x93', '≧', '］', \r\n",
    "        '\\x94', '∀', '♛', '\\x96', '∨', '◎', '↻', '⇩', '＜', '≫', '✩', '✪', '♕', '؟', '₤', '☛', '╮', '␊', '＋', '┈', '％', \r\n",
    "        '╋', '▽', '⇨', '┻', '⊗', '￡', '।', '▂', '✯', '▇', '＿', '➤', '✞', '＝', '▷', '△', '◙', '▅', '✝', '∧', '␉', '☭', \r\n",
    "        '┊', '╯', '☾', '➔', '∴', '\\x92', '▃', '↳', '＾', '׳', '➢', '╭', '➡', '＠', '⊙', '☢', '˝', '∏', '„', '∥', '❝', '☐', \r\n",
    "        '▆', '╱', '⋙', '๏', '☁', '⇔', '▔', '\\x91', '➚', '◡', '╰', '\\x85', '♢', '˙', '۞', '✘', '✮', '☑', '⋆', 'ⓘ', '❒', \r\n",
    "        '☣', '✉', '⌊', '➠', '∣', '❑', '◢', 'ⓒ', '\\x80', '〒', '∕', '▮', '⦿', '✫', '✚', '⋯', '♩', '☂', '❞', '‗', '܂', '☜', \r\n",
    "        '‾', '✜', '╲', '∘', '⟩', '＼', '⟨', '·', '✗', '♚', '∅', 'ⓔ', '◣', '͡', '‛', '❦', '◠', '✄', '❄', '∃', '␣', '≪', '｢', \r\n",
    "        '≅', '◯', '☽', '∎', '｣', '❧', '̅', 'ⓐ', '↘', '⚓', '▣', '˘', '∪', '⇢', '✍', '⊥', '＃', '⎯', '↠', '۩', '☰', '◥', \r\n",
    "        '⊆', '✽', '⚡', '↪', '❁', '☹', '◼', '☃', '◤', '❏', 'ⓢ', '⊱', '➝', '̣', '✡', '∠', '｀', '▴', '┤', '∝', '♏', 'ⓐ', \r\n",
    "        '✎', ';', '␤', '＇', '❣', '✂', '✤', 'ⓞ', '☪', '✴', '⌒', '˛', '♒', '＄', '✶', '▻', 'ⓔ', '◌', '◈', '❚', '❂', '￦', \r\n",
    "        '◉', '╜', '̃', '✱', '╖', '❉', 'ⓡ', '↗', 'ⓣ', '♻', '➽', '׀', '✲', '✬', '☉', '▉', '≒', '☥', '⌐', '♨', '✕', 'ⓝ', \r\n",
    "        '⊰', '❘', '＂', '⇧', '̵', '➪', '▁', '▏', '⊃', 'ⓛ', '‚', '♰', '́', '✏', '⏑', '̶', 'ⓢ', '⩾', '￠', '❍', '≃', '⋰', '♋', \r\n",
    "        '､', '̂', '❋', '✳', 'ⓤ', '╤', '▕', '⌣', '✸', '℮', '⁺', '▨', '╨', 'ⓥ', '♈', '❃', '☝', '✻', '⊇', '≻', '♘', '♞', \r\n",
    "        '◂', '✟', '⌠', '✠', '☚', '✥', '❊', 'ⓒ', '⌈', '❅', 'ⓡ', '♧', 'ⓞ', '▭', '❱', 'ⓣ', '∟', '☕', '♺', '∵', '⍝', 'ⓑ', \r\n",
    "        '✵', '✣', '٭', '♆', 'ⓘ', '∶', '⚜', '◞', '்', '✹', '➥', '↕', '̳', '∷', '✋', '➧', '∋', '̿', 'ͧ', '┅', '⥤', '⬆', '⋱', \r\n",
    "        '☄', '↖', '⋮', '۔', '♌', 'ⓛ', '╕', '♓', '❯', '♍', '▋', '✺', '⭐', '✾', '♊', '➣', '▿', 'ⓑ', '♉', '⏠', '◾', '▹', \r\n",
    "        '⩽', '↦', '╥', '⍵', '⌋', '։', '➨', '∮', '⇥', 'ⓗ', 'ⓓ', '⁻', '⎝', '⌥', '⌉', '◔', '◑', '✼', '♎', '♐', '╪', '⊚', \r\n",
    "        '☒', '⇤', 'ⓜ', '⎠', '◐', '⚠', '╞', '◗', '⎕', 'ⓨ', '☟', 'ⓟ', '♟', '❈', '↬', 'ⓓ', '◻', '♮', '❙', '♤', '∉', '؛', \r\n",
    "        '⁂', 'ⓝ', '־', '♑', '╫', '╓', '╳', '⬅', '☔', '☸', '┄', '╧', '׃', '⎢', '❆', '⋄', '⚫', '̏', '☏', '➞', '͂', '␙', \r\n",
    "        'ⓤ', '◟', '̊', '⚐', '✙', '↙', '̾', '℘', '✷', '⍺', '❌', '⊢', '▵', '✅', 'ⓖ', '☨', '▰', '╡', 'ⓜ', '☤', '∽', '╘', \r\n",
    "        '˹', '↨', '♙', '⬇', '♱', '⌡', '⠀', '╛', '❕', '┉', 'ⓟ', '̀', '♖', 'ⓚ', '┆', '⎜', '◜', '⚾', '⤴', '✇', '╟', '⎛', \r\n",
    "        '☩', '➲', '➟', 'ⓥ', 'ⓗ', '⏝', '◃', '╢', '↯', '✆', '˃', '⍴', '❇', '⚽', '╒', '̸', '♜', '☓', '➳', '⇄', '☬', '⚑', \r\n",
    "        '✐', '⌃', '◅', '▢', '❐', '∊', '☈', '॥', '⎮', '▩', 'ு', '⊹', '‵', '␔', '☊', '➸', '̌', '☿', '⇉', '⊳', '╙', 'ⓦ', \r\n",
    "        '⇣', '｛', '̄', '↝', '⎟', '▍', '❗', '״', '΄', '▞', '◁', '⛄', '⇝', '⎪', '♁', '⇠', '☇', '✊', 'ி', '｝', '⭕', '➘', \r\n",
    "        '⁀', '☙', '❛', '❓', '⟲', '⇀', '≲', 'ⓕ', '⎥', '\\u06dd', 'ͤ', '₋', '̱', '̎', '♝', '≳', '▙', '➭', '܀', 'ⓖ', '⇛', '▊', \r\n",
    "        '⇗', '̷', '⇱', '℅', 'ⓧ', '⚛', '̐', '̕', '⇌', '␀', '≌', 'ⓦ', '⊤', '̓', '☦', 'ⓕ', '▜', '➙', 'ⓨ', '⌨', '◮', '☷', \r\n",
    "        '◍', 'ⓚ', '≔', '⏩', '⍳', '℞', '┋', '˻', '▚', '≺', 'ْ', '▟', '➻', '̪', '⏪', '̉', '⎞', '┇', '⍟', '⇪', '▎', '⇦', '␝', \r\n",
    "        '⤷', '≖', '⟶', '♗', '̴', '♄', 'ͨ', '̈', '❜', '̡', '▛', '✁', '➩', 'ா', '˂', '↥', '⏎', '⎷', '̲', '➖', '↲', '⩵', '̗', '❢', \r\n",
    "        '≎', '⚔', '⇇', '̑', '⊿', '̖', '☍', '➹', '⥊', '⁁', '✢']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def remove_punctuation(text):\r\n",
    "    for punctuation in punctuation_list:\r\n",
    "        if punctuation in text:\r\n",
    "            text = text.replace(punctuation, f'{punctuation}')\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Limpeza de números"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def clean_numbers(text):\r\n",
    "    if bool(re.search(r'\\d', text)):\r\n",
    "        text = re.sub('[0-9]{5,}', '#####', text)\r\n",
    "        text = re.sub('[0-9]{4}', '####', text)\r\n",
    "        text = re.sub('[0-9]{3}', '###', text)\r\n",
    "        text = re.sub('[0-9]{2}', '##', text)\r\n",
    "    return text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remoção de Stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\r\n",
    "\r\n",
    "def remove_stopwords(text, is_lower_case=True):\r\n",
    "    tokenizer = ToktokTokenizer()\r\n",
    "    tokens = tokenizer.tokenize(text)\r\n",
    "    tokens = [token.strip() for token in tokens]\r\n",
    "    if is_lower_case:\r\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\r\n",
    "    else:\r\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\r\n",
    "    filtered_text = ' '.join(filtered_tokens)\r\n",
    "    return filtered_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Aplica todos os preprocessamentos \r\n",
    "def clean_text(x):\r\n",
    "    x = x.lower()\r\n",
    "    x = remove_punctuation(x)\r\n",
    "    x = clean_numbers(x)\r\n",
    "    x = remove_stopwords(x)\r\n",
    "    x = x.replace(\"'\", \"\")\r\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "df['preprocessed_text'] = df['Text'].apply(lambda x: clean_text(x))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8199 entries, 0 to 8198\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Text               8199 non-null   object \n",
      " 1   Classificacao      8199 non-null   object \n",
      " 2   num_words          8199 non-null   int64  \n",
      " 3   num_unique_words   8199 non-null   int64  \n",
      " 4   num_chars          8199 non-null   int64  \n",
      " 5   num_stopwords      8199 non-null   int64  \n",
      " 6   num_punctuations   8199 non-null   int64  \n",
      " 7   num_words_upper    8199 non-null   int64  \n",
      " 8   num_words_title    8199 non-null   int64  \n",
      " 9   mean_word_len      8199 non-null   float64\n",
      " 10  preprocessed_text  8199 non-null   object \n",
      "dtypes: float64(1), int64(7), object(3)\n",
      "memory usage: 704.7+ KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Construindo Vetorizadores e Modelos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import copy\r\n",
    "import time\r\n",
    "from sklearn.metrics._classification import log_loss\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer \r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "\r\n",
    "from sklearn import model_selection\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "from sklearn import metrics\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Count Vectorize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "dic_classificacao = {\r\n",
    "    'Neutro' : 0,\r\n",
    "    'Negativo': 0,\r\n",
    "    'Positivo': 1,\r\n",
    "}\r\n",
    "df['Classificacao'] = df['Classificacao'].replace(dic_classificacao)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "train, test = np.split(df.sample(frac=1), [int(.8*len(df))])\r\n",
    "train.shape, test.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((6559, 11), (1640, 11))"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# Creating CountVectorizer object\r\n",
    "vectorizer = CountVectorizer(\r\n",
    "    dtype=np.float32, \r\n",
    "    strip_accents='unicode', \r\n",
    "    analyzer='word',\r\n",
    "    token_pattern=r'\\w{1,}',\r\n",
    "    ngram_range=(1, 3),\r\n",
    "    min_df=3\r\n",
    ")\r\n",
    "# Fit the vectorizer on training data after preprocessing\r\n",
    "vectorizer.fit_transform(train['preprocessed_text'].values.tolist() + test['preprocessed_text'].values.tolist())\r\n",
    "train_vectorizer = vectorizer.transform(train['preprocessed_text'].values.tolist())\r\n",
    "test_vectorizer = vectorizer.transform(test['preprocessed_text'].values.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# For storing the threshold values and f1 score\r\n",
    "threshold_list = []\r\n",
    "best_f1_score_list = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Customizando função para construir o modelo e f1 score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "train_y = train['Classificacao'].values\r\n",
    "\r\n",
    "def buildModel(train_X, train_y, test_X, test_y, test_X2, model_obj):\r\n",
    "    model = copy.deepcopy(model_obj)\r\n",
    "    model.fit(train_X, train_y)\r\n",
    "    pred_test_y = model.predict_proba(test_X)[:,1]\r\n",
    "    pred_test_y2 = model.predict_proba(test_X2)[:,1]\r\n",
    "    return pred_test_y, pred_test_y2, model\r\n",
    "\r\n",
    "def best_threshold_function(val_y, pred_val_y):\r\n",
    "    threshold_dict = {}\r\n",
    "    for thresh in np.arange(0.1, 0.201, 0.01):\r\n",
    "        thresh = np.round(thresh, 2)\r\n",
    "        # Updating the dict with threshold as key and f1 score as value\r\n",
    "        threshold_dict[thresh] =  metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\r\n",
    "        \r\n",
    "    # Finding the max key\r\n",
    "    best_threshold = max(threshold_dict, key=threshold_dict.get)\r\n",
    "    \r\n",
    "    # finding the max value\r\n",
    "    best_f1_score = max(threshold_dict.values())\r\n",
    "    \r\n",
    "    print(f\"Best F1 Score: {best_f1_score} for threshold {best_threshold}\")\r\n",
    "    # Appending the f1 score and threshold for count vectorizer\r\n",
    "    threshold_list.append(best_threshold)\r\n",
    "    best_f1_score_list.append(best_f1_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regreção Logistica"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "cv_scores = []\r\n",
    "pred_full_test = 0\r\n",
    "# Creating a zero list equal to the shape of training data\r\n",
    "pred_train = np.zeros([train.shape[0]])\r\n",
    "\r\n",
    "# kfold with 5 n_splits\r\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\r\n",
    "\r\n",
    "for dev_index, val_index in kf.split(train):\r\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\r\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, LogisticRegression(C=5., solver='sag'))\r\n",
    "    pred_full_test = pred_full_test + pred_test_y\r\n",
    "    \r\n",
    "    # Updating the pred_train list with prediction value\r\n",
    "    pred_train[val_index] = pred_val_y\r\n",
    "    \r\n",
    "    # appending the cv scores\r\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n",
    "    break\r\n",
    "    \r\n",
    "best_threshold_function(val_y, pred_val_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best F1 Score: 0.9721706864564007 for threshold 0.2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "cv_scores = []\r\n",
    "pred_full_test = 0\r\n",
    "# Creating a zero list equal to the shape of training data\r\n",
    "pred_train = np.zeros([train.shape[0]])\r\n",
    "\r\n",
    "# kfold with 5 n_splits\r\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\r\n",
    "for dev_index, val_index in kf.split(train):\r\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\r\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, MultinomialNB())\r\n",
    "    pred_full_test = pred_full_test + pred_test_y\r\n",
    "    \r\n",
    "    # Updating the pred_train list with prediction value\r\n",
    "    pred_train[val_index] = pred_val_y\r\n",
    "    \r\n",
    "    # appending the cv scores\r\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n",
    "    break\r\n",
    "    \r\n",
    "best_threshold_function(val_y, pred_val_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best F1 Score: 0.9706422018348624 for threshold 0.19\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TFIDF Vextorizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3))\r\n",
    "vectorizer.fit_transform(train['preprocessed_text'].values.tolist() + test['preprocessed_text'].values.tolist())\r\n",
    "train_vectorizer = vectorizer.transform(train['preprocessed_text'].values.tolist())\r\n",
    "test_vectorizer = vectorizer.transform(test['preprocessed_text'].values.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "cv_scores = []\r\n",
    "pred_full_test = 0\r\n",
    "\r\n",
    "# Creating a zero list equal to the shape of training data\r\n",
    "pred_train = np.zeros([train.shape[0]])\r\n",
    "\r\n",
    "# kfold with 5 n_splits\r\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\r\n",
    "for dev_index, val_index in kf.split(train):\r\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\r\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, LogisticRegression(C=5., solver='sag'))\r\n",
    "    pred_full_test = pred_full_test + pred_test_y\r\n",
    "    \r\n",
    "    # Updating the pred_train list with prediction value\r\n",
    "    pred_train[val_index] = pred_val_y\r\n",
    "    \r\n",
    "     # appending the cv scores\r\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n",
    "    break\r\n",
    "    \r\n",
    "best_threshold_function(val_y, pred_val_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best F1 Score: 0.9532374100719424 for threshold 0.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "cv_scores = []\r\n",
    "pred_full_test = 0\r\n",
    "pred_train = np.zeros([train.shape[0]])\r\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\r\n",
    "for dev_index, val_index in kf.split(train):\r\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\r\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, MultinomialNB())\r\n",
    "    pred_full_test = pred_full_test + pred_test_y\r\n",
    "    pred_train[val_index] = pred_val_y\r\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n",
    "    break\r\n",
    "    \r\n",
    "best_threshold_function(val_y, pred_val_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best F1 Score: 0.886118038237739 for threshold 0.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hashing Vectorize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "vectorizer = HashingVectorizer(\r\n",
    "    dtype=np.float32,\r\n",
    "    strip_accents='unicode', \r\n",
    "    analyzer='word',\r\n",
    "    ngram_range=(1, 3),\r\n",
    "    n_features=2**10\r\n",
    ")\r\n",
    "vectorizer.fit_transform(train['preprocessed_text'].values.tolist() + test['preprocessed_text'].values.tolist())\r\n",
    "train_vectorizer = vectorizer.transform(train['preprocessed_text'].values.tolist())\r\n",
    "test_vectorizer = vectorizer.transform(train['preprocessed_text'].values.tolist())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "cv_scores = []\r\n",
    "pred_full_test = 0\r\n",
    "pred_train = np.zeros([train.shape[0]])\r\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\r\n",
    "for dev_index, val_index in kf.split(train):\r\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\r\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\r\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, LogisticRegression(C=5., solver='sag'))\r\n",
    "    pred_full_test = pred_full_test + pred_test_y\r\n",
    "    pred_train[val_index] = pred_val_y\r\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\r\n",
    "    break\r\n",
    "    \r\n",
    "best_threshold_function(val_y, pred_val_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best F1 Score: 0.9406474820143884 for threshold 0.2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparando todos os modelos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "from prettytable import PrettyTable\r\n",
    "    \r\n",
    "table = PrettyTable()\r\n",
    "vect = ([\"CountVectorizer\"] * 2) + ([\"TFIDFVectorizer\"] * 2) + ([\"HashingVectorizer\"])\r\n",
    "model = ([\"Logistic Regression\", \"Naive Bayes\"] * 2) + ([\"Logistic Regression\"])\r\n",
    "table.add_column(\"Model\", model)\r\n",
    "table.add_column(\"Vectorizer\", vect)\r\n",
    "table.add_column(\"Test F1-Score\", best_f1_score_list)\r\n",
    "table.add_column(\"Best Threshold\", threshold_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "print(table)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------------+-------------------+--------------------+----------------+\n",
      "|        Model        |     Vectorizer    |   Test F1-Score    | Best Threshold |\n",
      "+---------------------+-------------------+--------------------+----------------+\n",
      "| Logistic Regression |  CountVectorizer  | 0.9721706864564007 |      0.2       |\n",
      "|     Naive Bayes     |  CountVectorizer  | 0.9706422018348624 |      0.19      |\n",
      "| Logistic Regression |  TFIDFVectorizer  | 0.9532374100719424 |      0.2       |\n",
      "|     Naive Bayes     |  TFIDFVectorizer  | 0.886118038237739  |      0.2       |\n",
      "| Logistic Regression | HashingVectorizer | 0.9406474820143884 |      0.2       |\n",
      "+---------------------+-------------------+--------------------+----------------+\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "86b8d54e23acd62ae0afd56a257cc43973efd002b6e1dafe09c32756ffa9fd96"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}