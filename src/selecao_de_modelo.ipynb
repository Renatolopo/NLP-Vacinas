{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\renat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de Dados de Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8199, 26)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/train/Tweets_Mg.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de features inrelevantes\n",
    "df = df.drop(columns=['Unnamed: 0', 'Created At', 'Geo Coordinates.latitude',\n",
    "       'Geo Coordinates.longitude', 'User Location', 'Username',\n",
    "       'User Screen Name', 'Retweet Count', 'Observação',\n",
    "       'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13',\n",
    "       'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17',\n",
    "       'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21',\n",
    "       'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positivo    3300\n",
       "Neutro      2453\n",
       "Negativo    2446\n",
       "Name: Classificacao, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Classificacao.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ���⛪ @ Catedral de Santo Antônio - Governador ...\n",
       "1       � @ Governador Valadares, Minas Gerais https:/...\n",
       "2       �� @ Governador Valadares, Minas Gerais https:...\n",
       "3                             ��� https://t.co/BnDsO34qK0\n",
       "4       ��� PSOL vai questionar aumento de vereadores ...\n",
       "                              ...                        \n",
       "8194    Trio é preso suspeito de roubo, tráfico e abus...\n",
       "8195    Trio é preso suspeito de roubo, tráfico e abus...\n",
       "8196    Trio é preso suspeito de roubo, tráfico e abus...\n",
       "8197    Trio é preso suspeito de roubo, tráfico e abus...\n",
       "8198    Trio suspeito de roubo de cargas é preso em Sa...\n",
       "Name: Text, Length: 8199, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "### Features que serão adicionadas\n",
    "    1 - Número de palavras no texto\n",
    "    2 - Número de palavras únicas no texto\n",
    "    3 - Número de caracteres no texto\n",
    "    4 - Número de palavras irrelevantes (stopwords)\n",
    "    5 - Número de pontuações\n",
    "    6 - Número palavras maiúsculas\n",
    "    7 - Número de palavras de caixa de título\n",
    "    8 - Comprimento médio das palavras\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de palavras no texto\n",
    "df[\"num_words\"] = df['Text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Numero de palavras unicas no texto\n",
    "df['num_unique_words'] = df['Text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# Número de caracteres no texto\n",
    "df['num_chars'] = df['Text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Número de palavras irrelevantes (stopwords)\n",
    "df['num_stopwords'] = df['Text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n",
    "\n",
    "# Número de pontuações\n",
    "df['num_punctuations'] = df['Text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# Número palavras maiúsculas\n",
    "df['num_words_upper'] = df['Text'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "# Número de palavras de caixa de título\n",
    "df['num_words_title'] = df['Text'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "# Comprimento médio das palavras\n",
    "df['mean_word_len'] = df['Text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Classificacao</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>��� https://t.co/BnDsO34qK0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>126</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Classificacao  num_words  \\\n",
       "0  ���⛪ @ Catedral de Santo Antônio - Governador ...        Neutro         10   \n",
       "1  � @ Governador Valadares, Minas Gerais https:/...        Neutro          7   \n",
       "2  �� @ Governador Valadares, Minas Gerais https:...        Neutro          7   \n",
       "3                        ��� https://t.co/BnDsO34qK0        Neutro          2   \n",
       "4  ��� PSOL vai questionar aumento de vereadores ...      Negativo         20   \n",
       "\n",
       "   num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0                10         82              1                 8   \n",
       "1                 7         62              0                 7   \n",
       "2                 7         63              0                 7   \n",
       "3                 2         27              0                 5   \n",
       "4                17        126              5                 7   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                0                4       7.300000  \n",
       "1                0                4       8.000000  \n",
       "2                0                4       8.142857  \n",
       "3                0                0      13.000000  \n",
       "4                2                4       5.350000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8199 entries, 0 to 8198\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Text              8199 non-null   object \n",
      " 1   Classificacao     8199 non-null   object \n",
      " 2   num_words         8199 non-null   int64  \n",
      " 3   num_unique_words  8199 non-null   int64  \n",
      " 4   num_chars         8199 non-null   int64  \n",
      " 5   num_stopwords     8199 non-null   int64  \n",
      " 6   num_punctuations  8199 non-null   int64  \n",
      " 7   num_words_upper   8199 non-null   int64  \n",
      " 8   num_words_title   8199 non-null   int64  \n",
      " 9   mean_word_len     8199 non-null   float64\n",
      "dtypes: float64(1), int64(7), object(2)\n",
      "memory usage: 640.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Converter texto para minúsculo\n",
    "- Remover pontuações\n",
    "- Limpeza de Número \n",
    "- remoção de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuations\n",
    "punctuation_list =[',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n",
    "        '•', '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', \n",
    "        '█', '…', '“', '★', '”', '–', '●', '►', '−', '¢', '¬', '░', '¡', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', \n",
    "        '—', '‹', '─', '▒', '：', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', '¯', '♦', '¤', '▲', '¸', '⋅', '‘', '∞', \n",
    "        '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '・', '╦', '╣', '╔', '╗', '▬', '❤', '≤', '‡', '√', '◄', '━', \n",
    "        '⇒', '▶', '≥', '╝', '♡', '◊', '。', '✈', '≡', '☺', '✔', '↵', '≈', '✓', '♣', '☎', '℃', '◦', '└', '‟', '～', '！', '○', \n",
    "        '◆', '№', '♠', '▌', '✿', '▸', '⁄', '□', '❖', '✦', '．', '÷', '｜', '┃', '／', '￥', '╠', '↩', '✭', '▐', '☼', '☻', '┐', \n",
    "        '├', '«', '∼', '┌', '℉', '☮', '฿', '≦', '♬', '✧', '〉', '－', '⌂', '✖', '･', '◕', '※', '‖', '◀', '‰', '\\x97', '↺', \n",
    "        '∆', '┘', '┬', '╬', '،', '⌘', '⊂', '＞', '〈', '⎙', '？', '☠', '⇐', '▫', '∗', '∈', '≠', '♀', '♔', '˚', '℗', '┗', '＊', \n",
    "        '┼', '❀', '＆', '∩', '♂', '‿', '∑', '‣', '➜', '┛', '⇓', '☯', '⊖', '☀', '┳', '；', '∇', '⇑', '✰', '◇', '♯', '☞', '´', \n",
    "        '↔', '┏', '｡', '◘', '∂', '✌', '♭', '┣', '┴', '┓', '✨', '\\xa0', '˜', '❥', '┫', '℠', '✒', '［', '∫', '\\x93', '≧', '］', \n",
    "        '\\x94', '∀', '♛', '\\x96', '∨', '◎', '↻', '⇩', '＜', '≫', '✩', '✪', '♕', '؟', '₤', '☛', '╮', '␊', '＋', '┈', '％', \n",
    "        '╋', '▽', '⇨', '┻', '⊗', '￡', '।', '▂', '✯', '▇', '＿', '➤', '✞', '＝', '▷', '△', '◙', '▅', '✝', '∧', '␉', '☭', \n",
    "        '┊', '╯', '☾', '➔', '∴', '\\x92', '▃', '↳', '＾', '׳', '➢', '╭', '➡', '＠', '⊙', '☢', '˝', '∏', '„', '∥', '❝', '☐', \n",
    "        '▆', '╱', '⋙', '๏', '☁', '⇔', '▔', '\\x91', '➚', '◡', '╰', '\\x85', '♢', '˙', '۞', '✘', '✮', '☑', '⋆', 'ⓘ', '❒', \n",
    "        '☣', '✉', '⌊', '➠', '∣', '❑', '◢', 'ⓒ', '\\x80', '〒', '∕', '▮', '⦿', '✫', '✚', '⋯', '♩', '☂', '❞', '‗', '܂', '☜', \n",
    "        '‾', '✜', '╲', '∘', '⟩', '＼', '⟨', '·', '✗', '♚', '∅', 'ⓔ', '◣', '͡', '‛', '❦', '◠', '✄', '❄', '∃', '␣', '≪', '｢', \n",
    "        '≅', '◯', '☽', '∎', '｣', '❧', '̅', 'ⓐ', '↘', '⚓', '▣', '˘', '∪', '⇢', '✍', '⊥', '＃', '⎯', '↠', '۩', '☰', '◥', \n",
    "        '⊆', '✽', '⚡', '↪', '❁', '☹', '◼', '☃', '◤', '❏', 'ⓢ', '⊱', '➝', '̣', '✡', '∠', '｀', '▴', '┤', '∝', '♏', 'ⓐ', \n",
    "        '✎', ';', '␤', '＇', '❣', '✂', '✤', 'ⓞ', '☪', '✴', '⌒', '˛', '♒', '＄', '✶', '▻', 'ⓔ', '◌', '◈', '❚', '❂', '￦', \n",
    "        '◉', '╜', '̃', '✱', '╖', '❉', 'ⓡ', '↗', 'ⓣ', '♻', '➽', '׀', '✲', '✬', '☉', '▉', '≒', '☥', '⌐', '♨', '✕', 'ⓝ', \n",
    "        '⊰', '❘', '＂', '⇧', '̵', '➪', '▁', '▏', '⊃', 'ⓛ', '‚', '♰', '́', '✏', '⏑', '̶', 'ⓢ', '⩾', '￠', '❍', '≃', '⋰', '♋', \n",
    "        '､', '̂', '❋', '✳', 'ⓤ', '╤', '▕', '⌣', '✸', '℮', '⁺', '▨', '╨', 'ⓥ', '♈', '❃', '☝', '✻', '⊇', '≻', '♘', '♞', \n",
    "        '◂', '✟', '⌠', '✠', '☚', '✥', '❊', 'ⓒ', '⌈', '❅', 'ⓡ', '♧', 'ⓞ', '▭', '❱', 'ⓣ', '∟', '☕', '♺', '∵', '⍝', 'ⓑ', \n",
    "        '✵', '✣', '٭', '♆', 'ⓘ', '∶', '⚜', '◞', '்', '✹', '➥', '↕', '̳', '∷', '✋', '➧', '∋', '̿', 'ͧ', '┅', '⥤', '⬆', '⋱', \n",
    "        '☄', '↖', '⋮', '۔', '♌', 'ⓛ', '╕', '♓', '❯', '♍', '▋', '✺', '⭐', '✾', '♊', '➣', '▿', 'ⓑ', '♉', '⏠', '◾', '▹', \n",
    "        '⩽', '↦', '╥', '⍵', '⌋', '։', '➨', '∮', '⇥', 'ⓗ', 'ⓓ', '⁻', '⎝', '⌥', '⌉', '◔', '◑', '✼', '♎', '♐', '╪', '⊚', \n",
    "        '☒', '⇤', 'ⓜ', '⎠', '◐', '⚠', '╞', '◗', '⎕', 'ⓨ', '☟', 'ⓟ', '♟', '❈', '↬', 'ⓓ', '◻', '♮', '❙', '♤', '∉', '؛', \n",
    "        '⁂', 'ⓝ', '־', '♑', '╫', '╓', '╳', '⬅', '☔', '☸', '┄', '╧', '׃', '⎢', '❆', '⋄', '⚫', '̏', '☏', '➞', '͂', '␙', \n",
    "        'ⓤ', '◟', '̊', '⚐', '✙', '↙', '̾', '℘', '✷', '⍺', '❌', '⊢', '▵', '✅', 'ⓖ', '☨', '▰', '╡', 'ⓜ', '☤', '∽', '╘', \n",
    "        '˹', '↨', '♙', '⬇', '♱', '⌡', '⠀', '╛', '❕', '┉', 'ⓟ', '̀', '♖', 'ⓚ', '┆', '⎜', '◜', '⚾', '⤴', '✇', '╟', '⎛', \n",
    "        '☩', '➲', '➟', 'ⓥ', 'ⓗ', '⏝', '◃', '╢', '↯', '✆', '˃', '⍴', '❇', '⚽', '╒', '̸', '♜', '☓', '➳', '⇄', '☬', '⚑', \n",
    "        '✐', '⌃', '◅', '▢', '❐', '∊', '☈', '॥', '⎮', '▩', 'ு', '⊹', '‵', '␔', '☊', '➸', '̌', '☿', '⇉', '⊳', '╙', 'ⓦ', \n",
    "        '⇣', '｛', '̄', '↝', '⎟', '▍', '❗', '״', '΄', '▞', '◁', '⛄', '⇝', '⎪', '♁', '⇠', '☇', '✊', 'ி', '｝', '⭕', '➘', \n",
    "        '⁀', '☙', '❛', '❓', '⟲', '⇀', '≲', 'ⓕ', '⎥', '\\u06dd', 'ͤ', '₋', '̱', '̎', '♝', '≳', '▙', '➭', '܀', 'ⓖ', '⇛', '▊', \n",
    "        '⇗', '̷', '⇱', '℅', 'ⓧ', '⚛', '̐', '̕', '⇌', '␀', '≌', 'ⓦ', '⊤', '̓', '☦', 'ⓕ', '▜', '➙', 'ⓨ', '⌨', '◮', '☷', \n",
    "        '◍', 'ⓚ', '≔', '⏩', '⍳', '℞', '┋', '˻', '▚', '≺', 'ْ', '▟', '➻', '̪', '⏪', '̉', '⎞', '┇', '⍟', '⇪', '▎', '⇦', '␝', \n",
    "        '⤷', '≖', '⟶', '♗', '̴', '♄', 'ͨ', '̈', '❜', '̡', '▛', '✁', '➩', 'ா', '˂', '↥', '⏎', '⎷', '̲', '➖', '↲', '⩵', '̗', '❢', \n",
    "        '≎', '⚔', '⇇', '̑', '⊿', '̖', '☍', '➹', '⥊', '⁁', '✢']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for punctuation in punctuation_list:\n",
    "        if punctuation in text:\n",
    "            text = text.replace(punctuation, f'{punctuation}')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza de números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(text):\n",
    "    if bool(re.search(r'\\d', text)):\n",
    "        text = re.sub('[0-9]{5,}', '#####', text)\n",
    "        text = re.sub('[0-9]{4}', '####', text)\n",
    "        text = re.sub('[0-9]{3}', '###', text)\n",
    "        text = re.sub('[0-9]{2}', '##', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=True):\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica todos os preprocessamentos \n",
    "def clean_text(x):\n",
    "    x = x.lower()\n",
    "    x = remove_punctuation(x)\n",
    "    x = clean_numbers(x)\n",
    "    x = remove_stopwords(x)\n",
    "    x = x.replace(\"'\", \"\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['Text'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8199 entries, 0 to 8198\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Text               8199 non-null   object \n",
      " 1   Classificacao      8199 non-null   object \n",
      " 2   num_words          8199 non-null   int64  \n",
      " 3   num_unique_words   8199 non-null   int64  \n",
      " 4   num_chars          8199 non-null   int64  \n",
      " 5   num_stopwords      8199 non-null   int64  \n",
      " 6   num_punctuations   8199 non-null   int64  \n",
      " 7   num_words_upper    8199 non-null   int64  \n",
      " 8   num_words_title    8199 non-null   int64  \n",
      " 9   mean_word_len      8199 non-null   float64\n",
      " 10  preprocessed_text  8199 non-null   object \n",
      "dtypes: float64(1), int64(7), object(3)\n",
      "memory usage: 704.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construindo Vetorizadores e Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "from sklearn.metrics._classification import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_classificacao = {\n",
    "    'Neutro' : 1,\n",
    "    'Negativo': 0,\n",
    "    'Positivo': 1,\n",
    "}\n",
    "df['Classificacao'] = df['Classificacao'].replace(dic_classificacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6559, 11), (1640, 11))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = np.split(df.sample(frac=1), [int(.8*len(df))])\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CountVectorizer object\n",
    "vectorizer = CountVectorizer(\n",
    "    dtype=np.float32, \n",
    "    strip_accents='unicode', \n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=3\n",
    ")\n",
    "# Fit the vectorizer on training data after preprocessing\n",
    "vectorizer.fit_transform(train['preprocessed_text'].values.tolist() + test['preprocessed_text'].values.tolist())\n",
    "train_vectorizer = vectorizer.transform(train['preprocessed_text'].values.tolist())\n",
    "test_vectorizer = vectorizer.transform(test['preprocessed_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing the threshold values and f1 score\n",
    "threshold_list = []\n",
    "best_f1_score_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizando função para construir o modelo e f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train['Classificacao'].values\n",
    "\n",
    "def buildModel(train_X, train_y, test_X, test_y, test_X2, model_obj):\n",
    "    model = copy.deepcopy(model_obj)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)[:,1]\n",
    "    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def best_threshold_function(val_y, pred_val_y):\n",
    "    threshold_dict = {}\n",
    "    for thresh in np.arange(0.1, 0.201, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        # Updating the dict with threshold as key and f1 score as value\n",
    "        threshold_dict[thresh] =  metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "        \n",
    "    # Finding the max key\n",
    "    best_threshold = max(threshold_dict, key=threshold_dict.get)\n",
    "    \n",
    "    # finding the max value\n",
    "    best_f1_score = max(threshold_dict.values())\n",
    "    \n",
    "    print(f\"Best F1 Score: {best_f1_score} for threshold {best_threshold}\")\n",
    "    # Appending the f1 score and threshold for count vectorizer\n",
    "    threshold_list.append(best_threshold)\n",
    "    best_f1_score_list.append(best_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regreção Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.9802455953016552 for threshold 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "# Creating a zero list equal to the shape of training data\n",
    "pred_train = np.zeros([train.shape[0]])\n",
    "\n",
    "# kfold with 5 n_splits\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, LogisticRegression(C=5., solver='sag'))\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    \n",
    "    # Updating the pred_train list with prediction value\n",
    "    pred_train[val_index] = pred_val_y\n",
    "    \n",
    "    # appending the cv scores\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    break\n",
    "    \n",
    "best_threshold_function(val_y, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.9800323799244468 for threshold 0.18\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "# Creating a zero list equal to the shape of training data\n",
    "pred_train = np.zeros([train.shape[0]])\n",
    "\n",
    "# kfold with 5 n_splits\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, MultinomialNB())\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    \n",
    "    # Updating the pred_train list with prediction value\n",
    "    pred_train[val_index] = pred_val_y\n",
    "    \n",
    "    # appending the cv scores\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    break\n",
    "    \n",
    "best_threshold_function(val_y, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vextorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopwords, ngram_range=(1,3))\n",
    "vectorizer.fit_transform(train['preprocessed_text'].values.tolist() + test['preprocessed_text'].values.tolist())\n",
    "train_vectorizer = vectorizer.transform(train['preprocessed_text'].values.tolist())\n",
    "test_vectorizer = vectorizer.transform(test['preprocessed_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.9663865546218487 for threshold 0.19\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "\n",
    "# Creating a zero list equal to the shape of training data\n",
    "pred_train = np.zeros([train.shape[0]])\n",
    "\n",
    "# kfold with 5 n_splits\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, LogisticRegression(C=5., solver='sag'))\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    \n",
    "    # Updating the pred_train list with prediction value\n",
    "    pred_train[val_index] = pred_val_y\n",
    "    \n",
    "     # appending the cv scores\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    break\n",
    "    \n",
    "best_threshold_function(val_y, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.9729729729729729 for threshold 0.18\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0]])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, MultinomialNB())\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    break\n",
    "    \n",
    "best_threshold_function(val_y, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(\n",
    "    dtype=np.float32,\n",
    "    strip_accents='unicode', \n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 3),\n",
    "    n_features=2**10\n",
    ")\n",
    "vectorizer.fit_transform(train['preprocessed_text'].values.tolist() + test['preprocessed_text'].values.tolist())\n",
    "train_vectorizer = vectorizer.transform(train['preprocessed_text'].values.tolist())\n",
    "test_vectorizer = vectorizer.transform(train['preprocessed_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.9693446088794927 for threshold 0.2\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0]])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(train):\n",
    "    dev_X, val_X = train_vectorizer[dev_index], train_vectorizer[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = buildModel(dev_X, dev_y, val_X, val_y, test_vectorizer, LogisticRegression(C=5., solver='sag'))\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    break\n",
    "    \n",
    "best_threshold_function(val_y, pred_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando todos os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "table = PrettyTable()\n",
    "vect = ([\"CountVectorizer\"] * 2) + ([\"TFIDFVectorizer\"] * 2) + ([\"HashingVectorizer\"])\n",
    "model = ([\"Logistic Regression\", \"Naive Bayes\"] * 2) + ([\"Logistic Regression\"])\n",
    "table.add_column(\"Model\", model)\n",
    "table.add_column(\"Vectorizer\", vect)\n",
    "table.add_column(\"Test F1-Score\", best_f1_score_list)\n",
    "table.add_column(\"Best Threshold\", threshold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------------+--------------------+----------------+\n",
      "|        Model        |     Vectorizer    |   Test F1-Score    | Best Threshold |\n",
      "+---------------------+-------------------+--------------------+----------------+\n",
      "| Logistic Regression |  CountVectorizer  | 0.9802455953016552 |      0.14      |\n",
      "|     Naive Bayes     |  CountVectorizer  | 0.9800323799244468 |      0.18      |\n",
      "| Logistic Regression |  TFIDFVectorizer  | 0.9663865546218487 |      0.19      |\n",
      "|     Naive Bayes     |  TFIDFVectorizer  | 0.9729729729729729 |      0.18      |\n",
      "| Logistic Regression | HashingVectorizer | 0.9693446088794927 |      0.2       |\n",
      "+---------------------+-------------------+--------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86b8d54e23acd62ae0afd56a257cc43973efd002b6e1dafe09c32756ffa9fd96"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
